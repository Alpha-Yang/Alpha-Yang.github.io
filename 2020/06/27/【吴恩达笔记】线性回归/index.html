<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="吴恩达老师的机器学习公开课应该算是目前我见过，最适合小白入门的公开课程了。即使内容对新人比较友好，但我想还是需要整理些许笔记的，因为涉及到机器学习的基础阶段，各个概念的掌握情况，我跳过了一些对我个人而言不重要的栏目，比如线性代数和matlab教程这些。之后也会补上上课的作业，Coursera申请助学金需要时间比较久。 一元线性回归(单变量线性回归)  Linear Regression with">
<meta property="og:type" content="article">
<meta property="og:title" content="【吴恩达笔记】线性回归">
<meta property="og:url" content="http://yoursite.com/2020/06/27/%E3%80%90%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%94%E8%AE%B0%E3%80%91%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="杨文昊的个人博客">
<meta property="og:description" content="吴恩达老师的机器学习公开课应该算是目前我见过，最适合小白入门的公开课程了。即使内容对新人比较友好，但我想还是需要整理些许笔记的，因为涉及到机器学习的基础阶段，各个概念的掌握情况，我跳过了一些对我个人而言不重要的栏目，比如线性代数和matlab教程这些。之后也会补上上课的作业，Coursera申请助学金需要时间比较久。 一元线性回归(单变量线性回归)  Linear Regression with">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/image/ml1.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml2.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml3.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml4.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml5.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml6.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml7.jpg">
<meta property="og:image" content="http://yoursite.com/image/ml7.jpg">
<meta property="article:published_time" content="2020-06-27T14:58:06.000Z">
<meta property="article:modified_time" content="2020-06-28T12:16:35.667Z">
<meta property="article:author" content="Alpha Yang">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/image/ml1.jpg">

<link rel="canonical" href="http://yoursite.com/2020/06/27/%E3%80%90%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%94%E8%AE%B0%E3%80%91%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>【吴恩达笔记】线性回归 | 杨文昊的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="杨文昊的个人博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">杨文昊的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">愿你出走半生，归来仍是少年。</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/27/%E3%80%90%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%94%E8%AE%B0%E3%80%91%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1979(20200206-160439).JPG">
      <meta itemprop="name" content="Alpha Yang">
      <meta itemprop="description" content="世间万物，唯代码美食，不能辜负。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="杨文昊的个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【吴恩达笔记】线性回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-27 14:58:06" itemprop="dateCreated datePublished" datetime="2020-06-27T14:58:06Z">2020-06-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-28 12:16:35" itemprop="dateModified" datetime="2020-06-28T12:16:35Z">2020-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">吴恩达课程笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>吴恩达老师的机器学习公开课应该算是目前我见过，最适合小白入门的公开课程了。即使内容对新人比较友好，但我想还是需要整理些许笔记的，因为涉及到机器学习的基础阶段，各个概念的掌握情况，我跳过了一些对我个人而言不重要的栏目，比如线性代数和matlab教程这些。之后也会补上上课的作业，Coursera申请助学金需要时间比较久。</p>
<h3 id="一元线性回归-单变量线性回归-Linear-Regression-with-one-variable"><a href="#一元线性回归-单变量线性回归-Linear-Regression-with-one-variable" class="headerlink" title="一元线性回归(单变量线性回归)  Linear Regression with one variable"></a>一元线性回归(单变量线性回归)  Linear Regression with one variable</h3><a id="more"></a>
<h4 id="1-模型描述"><a href="#1-模型描述" class="headerlink" title="1. 模型描述"></a>1. 模型描述</h4><p>首先明确线性回归的作用，在机器学习中，我们可以广义的把机器学习划分为两类学习，监督学习与无监督学习。简单而言，监督学习就是给出了准确答案，期望计算机能学习其中的原理，并对我们未知的事情做出一定的预测。无监督学习就是指，我们并不知道准确结果的事情，希望机器通过学习，能从中抽取有效信息，举个简单的例子，例如邮箱垃圾邮件分类，新闻栏目划分等等，这些我们都能通过无监督学习去解决。</p>
<p>好了下面再举一个关于监督学习的例子，吴老师上课的例子是房价预估，如下图所示。</p>
<p><img src="/../image/ml1.jpg" alt=""></p>
<p>给你一定数量的数据集，代表房屋尺寸与价格的散点图，现在你希望知道除这些数据集以外的Size所对应的Price，相当于我之前说的预测。比如Size为1250时，那Price可能为2200左右这样子。那这个过程，我们实际就是想从现已知的数据集中学习某些知识内容，从而达到预测的效果。而这个过程就是我们所说的，<strong>回归</strong>。</p>
<p>其实就是拟合函数曲线，从而达到其与数据集的最优拟合，然后预测所有的Size。由于这个房子的单价这个假设中只与Size有关，所以我们称它为单变量或是一元。</p>
<p>下图是关于机器学习的基本流程图，你可以认为几乎所有机器学习问题都是这个框架的东西。</p>
<p><img src="/../image/ml2.jpg" alt=""></p>
<p>而这其中的$h$，代表$hypothesis$，由流程图可以看出，它是由数据集与学习算法得来的，在数学模型中，它代表回归模型，在机器学习中，我们通常把它称作假设函数。</p>
<script type="math/tex; mode=display">
\begin{align}
how\quad &to \quad represent \quad h \quad ?\\
h_{\theta}(x)&=\theta_0+\theta_1x
\end{align}</script><p>以上便是我们在一元线性回归模型中的假设函数了，可以看出，实际上我们是对数据集拟合了一个一次函数，所以我们才说是线性回归，那么实际的问题中，当然不是所有问题都是线性的关系，这个我们之后再去说。好了，目前为止，我已经解释了所有关于一元线性回归模型的意义了，下面我们来看点重要的。</p>
<h4 id="2-代价函数-cost-function"><a href="#2-代价函数-cost-function" class="headerlink" title="2. 代价函数 cost function"></a>2. 代价函数 cost function</h4><p><img src="/../image/ml3.jpg" alt=""></p>
<p>下面我们来看个更具体的例子，我们已经给出了数据集，要求求出Size和Price的内在关系，可以很明显的知道这是个一元回归问题，至于线性，这个我们不是我们现在需要考虑的，我们就当它拟合为线性一次函数。首先先解释数据集中几个变量符号的意义：</p>
<ul>
<li><strong>m：数据集的数量</strong></li>
<li><strong>x：特征/输入变量</strong>      $x^{(i)}$代表第i个特征$(i=1,2,…,m)$</li>
<li><strong>y：目标/输出变量</strong>      $y^{(i)}$代表第i个目标$(i=1,2,…,m)$</li>
</ul>
<p>当然不同的人有不同的表示方法，我相信这不是比较关键的东西，whatever。我们还是需要得到我们的假设函数$h_{\theta}(x)=\theta_0+\theta_1x$，如何选择$\theta_i$，即参数，才是我们需要关注的问题。ps：之后的$h_{\theta}(x)$全部简写为$h(x)$</p>
<p>我们的目的当然是，一次线性函数与原数据集有高度的拟合，即我们假设函数$h(x)$得到的结果与原目标$y$差距越小越好，于是我们得到了我们所谓的回归目标：</p>
<script type="math/tex; mode=display">
min_{\theta_i}:\frac{1}{2m}\sum_{i=1}^m \left(h(x^{(i)})-y^{(i)}\right)^2</script><p>这其实是个方差公式的形式，我们添上了系数1/2。然后很明显地，我们完全可以用这个函数来判断我们的拟合程度，这个值越小就说明我们的拟合效果越好。于是，我们就规定了一个函数：</p>
<script type="math/tex; mode=display">
J(\theta_i)=\frac{1}{2m}\sum_{i=1}^m \left(h(x^{(i)})-y^{(i)}\right)^2</script><p>然而这个函数就是<strong>代价函数(cost function)</strong>，表面意思就是为了拟合某个数据集，我们所需要付出的代价。在所有机器学习的问题中，我们都在研究如何使得代价函数最小，也就是我们上面所说的回归目标。</p>
<p><img src="/../image/ml4.jpg" alt=""></p>
<p>解决问题时，我们通常会有两张常见的可视化图片，第一张是$h(x)$的图，也就是Size和Price的数据集的图，第二张图是关于代价函数的，也有画三维图的，当然也有这种画等高线的。在接下来的问题中，我们会研究通过何种算法，让机器帮助我们找到最优目标的参数值。</p>
<h4 id="3-梯度下降算法-Gradient-Descent-Algorithm"><a href="#3-梯度下降算法-Gradient-Descent-Algorithm" class="headerlink" title="3. 梯度下降算法  Gradient Descent Algorithm"></a>3. 梯度下降算法  Gradient Descent Algorithm</h4><p>再次明确我们的模型函数，我们现在有$J(\theta_i)$,目标是让这个函数得到最小值。朴素的思想就是我们可以定起始的$\theta_i$，然后不断地改变他们，直到他们最后到达了最小值。很明显，这样的想法我们还需要更可靠的算法，毕竟全部遍历是不可能的。</p>
<p>所以这也就引入了梯度下降算法来解决这个问题，先给大家看某个代价函数，以及算法实现的某个过程。</p>
<p><img src="/../image/ml5.jpg" alt=""></p>
<p>想法不变，我们从某个$(\theta_0,\theta_1)$开始，不断改变他们的值，直到收敛到局部最优。而这个改变的过程我们称为梯度下降，为了方便理解，我直接给出参数改变的公式，也就是$\theta_i$不断更新的依据，假设仅有两个参数的时候。</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha \frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1) \quad (j=0,1)</script><p>这便是梯度下降最关键的地方，其中有了偏导数符号，这个如果看不懂的话，还是要先学微积分的。仔细想想的话，这样确实能通过下降的方式，使得代价函数收敛到局部最优解。其中$\alpha$代表学习率，你可以理解为每次更新的步长，这个也很关键，但是之后我们也会提到。</p>
<p>还有需要强调的是，我们的$\theta_i$是同步更新(Simultaneous update)的，这个如果你学过计算方法这门课中求解微分方程的两种方法或许你应该深有体会。</p>
<p>下面这个例子是同步更新</p>
<script type="math/tex; mode=display">
\begin{align}\\
Correct&:Simultaneous \quad update\\
temp0&:=\theta_0-\alpha \frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)\\
temp1&:=\theta_1-\alpha \frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)\\
\theta_0&:=temp0\\
\theta_1&:=temp1
\end{align}</script><p>然后这是不同步更新的例子</p>
<script type="math/tex; mode=display">
\begin{align}
Inco&rrect:\\
temp0&:=\theta_0-\alpha \frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)\\
\theta_0&:=temp0\\
temp1&:=\theta_1-\alpha \frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)\\
\theta_1&:=temp1
\end{align}</script><p>在不同步更新的例子中我们发现，$\theta_0$更新后带入了$\theta_1$的更新公式中，这不是我们想要的。</p>
<p>理解清楚了这个后，我们就明白了梯度下降法的公式了，你可能会说选择不同的开始的起始值会导致不同的收敛点，确实是这样，比如下面这个图。</p>
<p><img src="/../image/ml6.jpg" alt=""></p>
<p>很显然因为选择了不同的$\theta_i$会导致两种不同的路线，通过梯度下降算法我们只能找到局部收敛点，而不是全局最优点。不过这也没关系，我们处理问题的时候都尽量会选择弓形函数，也就是<strong>凸函数(convex function)</strong>.这种函数我们保证它仅有一个收敛点，即全局最优解。</p>
<h3 id="多元线性回归-Linear-Regression-with-Multiple-Variables"><a href="#多元线性回归-Linear-Regression-with-Multiple-Variables" class="headerlink" title="多元线性回归  Linear Regression with Multiple Variables"></a>多元线性回归  Linear Regression with Multiple Variables</h3><h4 id="1-模型描述-多特征"><a href="#1-模型描述-多特征" class="headerlink" title="1. 模型描述 多特征"></a>1. 模型描述 多特征</h4><p>我们根据上面那个房价的例子，很显然在实际的生活中，房价Price绝不可能只与其Size有关系的，你的北京六环和二环的房子能比得起来吗？哦，我知道你没有。这里还是根据那个房价的例子，不过我们提出了更多的影响因素。</p>
<p><img src="/../image/ml7.jpg" alt=""></p>
<p>我们还是首先来规定下符号：</p>
<ul>
<li><strong>m：数据集的数量</strong></li>
<li><strong>n：特征的个数</strong>  例如此例n=4</li>
<li><strong>$x^{(i)}$：序号为i的数据集的特征向量</strong></li>
<li><strong>$x_j^{(i)}$：序号为i的数据集的第j个特征</strong>  比如40年我们如何表示？ $x_4^{(2)}$=40</li>
</ul>
<p>假设函数那当然也是多变量的了，我默认大家都学完了矩阵的东西，所以我就直接用矩阵的表示形式来列了。</p>
<script type="math/tex; mode=display">
\begin{align}
h(x)&=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\\
define \quad x_0&=1\\
x=\begin{bmatrix} x_0\\x_1\\x_2\\\vdots\\x_n \end{bmatrix} \quad& \theta=\begin{bmatrix} \theta_0\\\theta_1\\\theta_2\\\vdots\\\theta_n \end{bmatrix}\\
h(x)&=\theta^Tx
\end{align}</script><p>我们采用矩阵的表达方式，已经写出了假设函数的矩阵形式，很简单，那下面就来说说如何在这个假设函数中进行梯度下降。</p>
<h4 id="2-多变量梯度下降"><a href="#2-多变量梯度下降" class="headerlink" title="2. 多变量梯度下降"></a>2. 多变量梯度下降</h4><p>其实梯度下降的公式都是一样的，只不过原来一元的时候只需要更新两个$\theta$值，现在需要更新n个，仅此而已。</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha \frac{\partial}{\partial \theta_j}J(\theta) \quad (j=0,1,...,n)</script><p>但是我们需要注意的，可不是这些。</p>
<h5 id="1-特征缩放"><a href="#1-特征缩放" class="headerlink" title="(1) 特征缩放"></a>(1) 特征缩放</h5><p>线性回归的目的就是在于求出我们所需要的权重，即$\theta$值，而权重和每个特征量的大小相关。试想一下，如果一个房间的Size的范围是1000左右，而房子的楼层是3以内的数，那我们直接拟合出来的权重，肯定更偏向于Size啊，而这显然不符合实际情况。这就是多元回归区别于一元回归的事情，一元回归就一个变量，咱们不需要考虑其大小范围。但是多元我们必须考虑，我们最好能够将所有变量统一缩放到某个区间，比如$-1\leq x_i\leq 1$。当然这个区间不是必须的，这里只给个参考范围。而对于所有的变量，我们都需要先进行<strong>均值标准化(Mean Normalization)</strong> 的处理。</p>
<script type="math/tex; mode=display">
\begin{align}
Mean &\quad Normalization:\\
x_j&:=\frac{x_j-\mu_j}{\sigma_j}
\end{align}</script><p>我相信如果你学过概率论的话，这个式子应该经常出现，不管是在八大公式还是假设检验里面。还是需要解释一下，这其中$\mu_j$表示第j组特征向量的均值，而$\sigma_j$表示第j组数据的标准差。</p>
<h5 id="2-特征合并"><a href="#2-特征合并" class="headerlink" title="(2) 特征合并"></a>(2) 特征合并</h5><p>这个是很好理解，也算是多元线性回归里面的一点技巧。比如题目给出的因素中，Price与房子的frontage和depth有关，那我们就要想到$Size=Frontage\times Depth$。于是我们就能把这个两个特征当成一个来处理，其实实际问题中这样的例子还是挺常见的。</p>
<h5 id="3-学习率与假设函数选择"><a href="#3-学习率与假设函数选择" class="headerlink" title="(3) 学习率与假设函数选择"></a>(3) 学习率与假设函数选择</h5><p>关于学习率$\alpha$与假设函数次项的选择，我在这里就不明说了，因为我自己也不是很能给出一个具体的方案。以及这样的情况遇到的较少。</p>
<h3 id="正规方程求解参数-Normal-Equation"><a href="#正规方程求解参数-Normal-Equation" class="headerlink" title="正规方程求解参数  Normal Equation"></a>正规方程求解参数  Normal Equation</h3><p>事实上，我们除了梯度下降法，还有另一种方法求解我们所需要的参数以及权重$\theta$的方法，就是用普通的方程求解，剩下的交给MATLAB就好，以上面那个例子来说明。</p>
<p><img src="/../image/ml7.jpg" alt=""></p>
<p>我们希望求得一组$\theta$值，能够假设函数能够很好地拟合最终的Price，其实我们不难列出我们的假设函数：</p>
<script type="math/tex; mode=display">
h(x)=
\begin{bmatrix}
1 & 2104 & 5 &1 &45\\
1 & 1416 & 3 &2 &40\\
\vdots & \vdots & \vdots & \vdots & \vdots
\end{bmatrix}
\times
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\vdots \\
\theta_n
\end{bmatrix}=X\theta</script><p>实际上我们添加了$x_0=1$这一项，如果能够理解的话，那说明你基本明白了线性回归的知识点。然后我们要用Normal的方法解得我们需要的$\theta$：</p>
<script type="math/tex; mode=display">
\theta = (X^TX)^{-1}X^Ty \quad ,\quad y=\begin{bmatrix} 460\\232\\315\\178\\ \vdots\end{bmatrix}</script><p>很遗憾地告诉你，我并不能告诉你这个方程是怎么推出来的，其实你也不需要理解。在数学建模竞赛或是机器学习研究中，我们大部分情况下只需要会用即可。</p>
<p>ok，到这里关于线性回归模型的求解，你已经学会了两种方法，梯度下降算法与正规方程解法，你可能会觉得正规方程解法会比较方便，下面我就来说说正规方程解法的局限性。</p>
<ul>
<li>正规方程解法需要计算矩阵的逆，当特征数大于样本数时，矩阵为奇异矩阵，无逆矩阵，虽然MATLAB命令<code>pinv</code>也能得到近似解，但是这往往不符合我们的预测要求。</li>
<li>当样本数量也特别大的时候，矩阵计算会非常缓慢，通常几千以内的矩阵，MATLAB计算还是比梯度下降快的，数量再往上我们可能就要选择梯度下降法了。</li>
</ul>
<p>当然梯度下降法也要选择学习率与迭代次数，这就需要我们理解代价函数，并根据此来判断。Emmm，说到这里，线性回归的内容基本就结束了，我可能会遗憾的告诉你，大部分数学建模竞赛或是机器学习问题，光靠线性回归模型基本不可能解决问题，因为真正的实际问题是线性关系的真的很少，但是这并不影响线性回归依旧是你最需要掌握的算法，这是所有其他问题的基础。</p>
<p>Good Luck！</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

            <div class="social-item">
              <a target="_blank" class="social-link" href="/atom.xml">
                <span class="icon">
                  <i class="fa fa-rss"></i>
                </span>

                <span class="label">RSS</span>
              </a>
            </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/25/%E3%80%90XDUACM%E6%9A%91%E5%9F%B9%E3%80%91%E4%BA%8C%E5%88%86%E6%B3%95%E6%B4%9B%E8%B0%B7%E4%BD%9C%E4%B8%9A%E9%A2%98%E8%A7%A3/" rel="prev" title="【XDUACM暑培】二分法洛谷作业题解">
      <i class="fa fa-chevron-left"></i> 【XDUACM暑培】二分法洛谷作业题解
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/29/%E3%80%90%E5%88%9D%E7%AD%89%E6%A8%A1%E5%9E%8B%E3%80%91%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="next" title="【初等模型】线性回归">
      【初等模型】线性回归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#一元线性回归-单变量线性回归-Linear-Regression-with-one-variable"><span class="nav-number">1.</span> <span class="nav-text">一元线性回归(单变量线性回归)  Linear Regression with one variable</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-模型描述"><span class="nav-number">1.1.</span> <span class="nav-text">1. 模型描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-代价函数-cost-function"><span class="nav-number">1.2.</span> <span class="nav-text">2. 代价函数 cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-梯度下降算法-Gradient-Descent-Algorithm"><span class="nav-number">1.3.</span> <span class="nav-text">3. 梯度下降算法  Gradient Descent Algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多元线性回归-Linear-Regression-with-Multiple-Variables"><span class="nav-number">2.</span> <span class="nav-text">多元线性回归  Linear Regression with Multiple Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-模型描述-多特征"><span class="nav-number">2.1.</span> <span class="nav-text">1. 模型描述 多特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-多变量梯度下降"><span class="nav-number">2.2.</span> <span class="nav-text">2. 多变量梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-特征缩放"><span class="nav-number">2.2.1.</span> <span class="nav-text">(1) 特征缩放</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-特征合并"><span class="nav-number">2.2.2.</span> <span class="nav-text">(2) 特征合并</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-学习率与假设函数选择"><span class="nav-number">2.2.3.</span> <span class="nav-text">(3) 学习率与假设函数选择</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正规方程求解参数-Normal-Equation"><span class="nav-number">3.</span> <span class="nav-text">正规方程求解参数  Normal Equation</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Alpha Yang"
      src="/images/IMG_1979(20200206-160439).JPG">
  <p class="site-author-name" itemprop="name">Alpha Yang</p>
  <div class="site-description" itemprop="description">世间万物，唯代码美食，不能辜负。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Alpha-Yang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Alpha-Yang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>


<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
      </div>
	  
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alpha Yang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <script async src="/js/cursor/fireworks.js"></script>



</body>
</html>
