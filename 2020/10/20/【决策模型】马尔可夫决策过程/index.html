<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="咕了2个月的时光，我终于开始更新这个专栏系列了，真的惭愧，这两天在忙着整理校长奖学金的材料，所以写完这篇文章我可能需要再咕一段时间QAQ。 这次来讲一个数学模型中常见但是非常难以理解的话题，马尔可夫决策过程。可能很多同学都听过这个，那么这篇文章将带你搞懂马尔可夫性、马尔可夫链、马尔可夫过程、马尔可夫决策过程这些概念。由于本章内容有些难以理解，所以我前前后后也研究了一个多月。由于我目前的研究兴趣在于">
<meta property="og:type" content="article">
<meta property="og:title" content="【决策模型】马尔可夫决策过程">
<meta property="og:url" content="http://yoursite.com/2020/10/20/%E3%80%90%E5%86%B3%E7%AD%96%E6%A8%A1%E5%9E%8B%E3%80%91%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/index.html">
<meta property="og:site_name" content="杨文昊的个人博客">
<meta property="og:description" content="咕了2个月的时光，我终于开始更新这个专栏系列了，真的惭愧，这两天在忙着整理校长奖学金的材料，所以写完这篇文章我可能需要再咕一段时间QAQ。 这次来讲一个数学模型中常见但是非常难以理解的话题，马尔可夫决策过程。可能很多同学都听过这个，那么这篇文章将带你搞懂马尔可夫性、马尔可夫链、马尔可夫过程、马尔可夫决策过程这些概念。由于本章内容有些难以理解，所以我前前后后也研究了一个多月。由于我目前的研究兴趣在于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/image/markov1.png">
<meta property="og:image" content="http://yoursite.com/image/markov2.png">
<meta property="og:image" content="http://yoursite.com/image/markov3.png">
<meta property="og:image" content="http://yoursite.com/image/markov4.png">
<meta property="og:image" content="http://yoursite.com/image/markov5.png">
<meta property="og:image" content="http://yoursite.com/image/markov6.png">
<meta property="og:image" content="http://yoursite.com/image/markov7.png">
<meta property="og:image" content="http://yoursite.com/image/markov8.png">
<meta property="og:image" content="http://yoursite.com/image/markov9.png">
<meta property="og:image" content="http://yoursite.com/image/markov10.png">
<meta property="og:image" content="http://yoursite.com/image/markov11.png">
<meta property="og:image" content="http://yoursite.com/image/markov12.png">
<meta property="og:image" content="http://yoursite.com/image/markov13.png">
<meta property="article:published_time" content="2020-10-19T16:45:09.000Z">
<meta property="article:modified_time" content="2020-11-06T16:56:01.654Z">
<meta property="article:author" content="Alpha Yang">
<meta property="article:tag" content="马尔可夫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/image/markov1.png">

<link rel="canonical" href="http://yoursite.com/2020/10/20/%E3%80%90%E5%86%B3%E7%AD%96%E6%A8%A1%E5%9E%8B%E3%80%91%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>【决策模型】马尔可夫决策过程 | 杨文昊的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="杨文昊的个人博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">杨文昊的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">愿你出走半生，归来仍是少年。</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/20/%E3%80%90%E5%86%B3%E7%AD%96%E6%A8%A1%E5%9E%8B%E3%80%91%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1979(20200206-160439).JPG">
      <meta itemprop="name" content="Alpha Yang">
      <meta itemprop="description" content="世间万物，唯代码美食，不能辜负。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="杨文昊的个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【决策模型】马尔可夫决策过程
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-19 16:45:09" itemprop="dateCreated datePublished" datetime="2020-10-19T16:45:09Z">2020-10-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-06 16:56:01" itemprop="dateModified" datetime="2020-11-06T16:56:01Z">2020-11-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" itemprop="url" rel="index">
                    <span itemprop="name">数学建模</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E6%A8%A1%E5%9E%8B%E7%AF%87/" itemprop="url" rel="index">
                    <span itemprop="name">模型篇</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>咕了2个月的时光，我终于开始更新这个专栏系列了，真的惭愧，这两天在忙着整理校长奖学金的材料，所以写完这篇文章我可能需要再咕一段时间QAQ。</p>
<p>这次来讲一个数学模型中常见但是非常难以理解的话题，马尔可夫决策过程。可能很多同学都听过这个，那么这篇文章将带你搞懂马尔可夫性、马尔可夫链、马尔可夫过程、马尔可夫决策过程这些概念。由于本章内容有些难以理解，所以我前前后后也研究了一个多月。由于我目前的研究兴趣在于强化学习，而马尔可夫决策过程是强化学习建模的最基本模型，所以本文是根据David Silver的强化学习公开课整理的资料，马尔可夫决策过程是非常重要的模型。</p>
<a id="more"></a>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ul>
<li>马尔可夫决策过程描述了我们想去决策事件本身的环境，即environment，我们的决策会因为环境得改变而改变，如果智能体agent选择的决策使得环境变差了，那它就会为此背锅，整个环境是持续可观察的。</li>
<li>目前的马尔可夫决策过程，可用于相当多的行为以及模式的决策分析，也在扩展人工智能的边界，在数学建模的问题中，也可改进很多的模型。</li>
<li>本文只讲解马尔可夫决策过程的模型分析以及概念解读与公式推导，关于模型的求解方法可利用动态规划、随机采样等，在数学建模中没必要运用深度学习法。</li>
</ul>
<h2 id="2-Markov-Processes马尔可夫过程"><a href="#2-Markov-Processes马尔可夫过程" class="headerlink" title="2 Markov Processes马尔可夫过程"></a>2 Markov Processes马尔可夫过程</h2><h3 id="2-1-Markov-Property马尔可夫性"><a href="#2-1-Markov-Property马尔可夫性" class="headerlink" title="2.1 Markov Property马尔可夫性"></a>2.1 Markov Property马尔可夫性</h3><p>在了解马尔可夫过程之前，我们首先得了解什么是马尔可夫性，马尔可夫性其实是一种假设，“未来的一切仅与现在有关，独立于过去的状态”。</p>
<p>关于马尔可夫性，我们给出了如下的Definition：</p>
<script type="math/tex; mode=display">
A\; state\; S_t\; is\; Markov\; if\; and\; only\; if\\
\mathbb{P}[S_{t+1}\;|\; S_t]=\mathbb{P}[S_{t+1}\; |\; S_1,...,S_t]</script><p>从上述的式子可以看出，t+1时刻的状态包含了1,..,t时刻状态的全部历史信息，并且当我们知道t时刻的状态后，我们只关注于环境的信息，而不用管之前所有状态的信息，这就是马尔可夫性，当论文中说某一状态或其他信息符合马尔可夫性时，我们也应当联想到这个性质。</p>
<h3 id="2-2-State-Transition-Matrix状态传输矩阵"><a href="#2-2-State-Transition-Matrix状态传输矩阵" class="headerlink" title="2.2 State Transition Matrix状态传输矩阵"></a>2.2 State Transition Matrix状态传输矩阵</h3><p>对于当前的马尔可夫状态s和其他的状态s’，状态传输矩阵的Definition：</p>
<script type="math/tex; mode=display">
\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'\; |\; S_t=s]\\
\mathcal{P}=
\begin{bmatrix}
\mathcal{P}_{11} & \ldots & \mathcal{P}_{1n}\\
\vdots & & \vdots \\
\mathcal{P}_{n1} & \ldots & \mathcal{P}_{nn}\\
\end{bmatrix}</script><p>以上就是状态传输矩阵的定义，大部分的模型建立都是利用矩阵的运算的，所以这部分很重要，当然$\Sigma\mathcal{P_{ss’}}=1$，这相信比较好理解。</p>
<h3 id="2-3-Markov-Chain马尔可夫链"><a href="#2-3-Markov-Chain马尔可夫链" class="headerlink" title="2.3 Markov Chain马尔可夫链"></a>2.3 Markov Chain马尔可夫链</h3><p>马尔可夫链(Markov Chain)又称马尔可夫过程(Markov Process)，是一种无记忆的随机过程(memoryless random process)，我们给出如下Definition，马尔可夫链是状态与转移概率的组合，$A\; Markov\; Chain\; is\; a\; tuple\; \langle \mathcal{S},\mathcal{P} \rangle$</p>
<p>其中，状态$\mathcal{S}$是状态的集合，概率$\mathcal{P}$是概率的矩阵。</p>
<p>下面我们用David老师上课举的一个例子来进一步理解马尔可夫过程，注意这个例子贯穿了整堂课程的内容。</p>
<p><img src="/../image/markov1.png" alt=""></p>
<p>这是一个学生上课情况的马尔可夫链结构，我们将Class 1作为学生的起始状态，把Sleep作为学生的终止状态。那我们可以想到一个学生从开始到结束可能会经历各种不同可能的状态。可能刷Facebook上瘾了出不来了，可能去Pub蹦迪了又返回复习了第一节课，等等。</p>
<p>那我们接下来首先写出这条马尔可夫链的状态传输矩阵：</p>
<script type="math/tex; mode=display">
\mathcal{P}=
\begin{bmatrix}
& 0.5 & & & & 0.5 & \\
& & 0.8 & & & & 0.2\\
& &  & 0.6 & 0.4 & & \\
& &  & & & & 1.0\\
0.2 & 0.4 & 0.4 & & & & \\
0.1 & & & & & 0.9 & \\
& & & & & & 1\\
\end{bmatrix}</script><p>其中行列分别代表状态C1,C2,C3,Pass,Pub,Facebook,Sleep。</p>
<h2 id="3-Markov-Reward-Process马尔可夫奖励过程"><a href="#3-Markov-Reward-Process马尔可夫奖励过程" class="headerlink" title="3 Markov Reward Process马尔可夫奖励过程"></a>3 Markov Reward Process马尔可夫奖励过程</h2><h3 id="3-1-MRP"><a href="#3-1-MRP" class="headerlink" title="3.1 MRP"></a>3.1 MRP</h3><p>简单来说，马尔可夫奖励过程就是含有奖励的马尔可夫链，要想理解MRP方程的含义，我们就得弄清楚奖励函数的由来，我们可以把奖励表述为进入某一状态后收获的奖励。奖励函数如下所示：</p>
<script type="math/tex; mode=display">
\mathcal{R}_s=\mathbb{E}[R_{t+1}\; | \; S_t=s]</script><p>其实看到这个公式，我相信很多读者应该和我一样纠结，为什么奖励是t+1下一时刻的呢？然而这只是一个约定而已，实际上你把$R_{t+1}$改成$R_t$也没什么关系。本质上还是我们上面说的“把奖励表述为进入某一状态后收获的奖励”，这样理解即可。</p>
<p>下面就能给出MRP的Definition了，$A\; Markov\; reward\; process\;is\; a\; tuple\; \langle \mathcal{S},\mathcal{P},\mathcal{R},\mathcal{\gamma} \rangle$</p>
<p>其中，S还是状态合集，P是概率传输矩阵，R是奖励函数如上所示，$\gamma$是衰减因子。</p>
<p>衰减因子是金融学上的概念，代表了对于远期利益的不确定性，其中$\gamma \in[0,1]$，下面讲到回报时还会提到。</p>
<h3 id="3-2-Return回报"><a href="#3-2-Return回报" class="headerlink" title="3.2 Return回报"></a>3.2 Return回报</h3><p>回报的定义是从当前时刻开始的回报与衰减因子的乘积之和，公式如下：</p>
<script type="math/tex; mode=display">
G_t = R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^k R_{t+k+1}</script><p>注意回报的定义并不是当前状态之前的奖励总和，我们一定要理解马尔科夫模型构建的意义，是为了探寻未来的最优策略，以及马尔可夫性与历史总是不相关的，仅与当前状态有关。所以一切模型构建均是围绕未来进行展开的，包括这里的回报。衰减因子代表人们对于未来奖励的期望，如果$\gamma$趋近0，则更重视眼前的利益，如果$\gamma$趋近1，则更重视未来的利益。</p>
<h3 id="3-3-Value-Function价值函数"><a href="#3-3-Value-Function价值函数" class="headerlink" title="3.3 Value Function价值函数"></a>3.3 Value Function价值函数</h3><p>价值函数的定义是当处于现在状态s时，MRP未来回报的期望值，价值函数给出了当前状态的长期价值。</p>
<script type="math/tex; mode=display">
v(s)=\mathbb{E}[G_t\; |\; S_t=s]</script><p>下面说了这么多概念让我们回到刚刚学生上课的例子。</p>
<p><img src="/../image/markov2.png" alt=""></p>
<p>在上图中，圆圈代表状态，R代表进入圆圈状态的即时奖励，而圆圈内的红色数字代表的就是价值函数，如何求出价值函数也就是当前马尔可夫最重要也是最需要讨论的问题。</p>
<h3 id="3-4-Bellman-Equation贝尔曼方程"><a href="#3-4-Bellman-Equation贝尔曼方程" class="headerlink" title="3.4 Bellman Equation贝尔曼方程"></a>3.4 Bellman Equation贝尔曼方程</h3><p>要想求解马尔可夫奖励过程的价值函数，我们在这里引入了贝尔曼方程，首先让我们看看贝尔曼方程：</p>
<script type="math/tex; mode=display">
\begin{align}
v(s) &= \mathbb{E}[G_t\; |\;S_t=s]\\
& = \mathbb{E}[R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}+\cdots \; | \; S_t=s]\\
& = \mathbb{E}[R_{t+1} + \gamma (R_{t+2} +\gamma R_{t+3}+\cdots \;) | \; S_t=s]\\
& = \mathbb{E}[R_{t+1} + \gamma G_{t+1}\; |\; S_t=s]\\
& = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})\; |\; S_t=s]
\end{align}</script><p>这就是贝尔曼方程简单的推导过程，这样我们就可以把价值函数分为两部分，一部分是即时奖励R，一部分是计算损失的下一状态的价值函数。</p>
<p>下面，我们抛开时间的关系，仅由状态来列写上述方程：</p>
<script type="math/tex; mode=display">
v(s) = \mathbb{E}[R_{t+1}+\gamma v(S_{t+1})\; |\; S_t=s]\\
v(s) = R_s + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}v(s')</script><p>以上两式就是贝尔曼方程的两种形式了，下面我们来进行求解，将贝尔曼方程简化为矩阵形式：</p>
<script type="math/tex; mode=display">
v = \mathcal{R}+\gamma \mathcal{P}v\\
\begin{bmatrix}
v(1)\\
\vdots\\
v(n)
\end{bmatrix}
=
\begin{bmatrix}
\mathcal{R}_1\\
\vdots\\
\mathcal{R}_n
\end{bmatrix}
+
\gamma
\begin{bmatrix}
\mathcal{P}_{11}&\ldots&\mathcal{P}_{1n}\\
\vdots\\
\mathcal{R}_n&\ldots&\mathcal{P}_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1)\\
\vdots\\
v(n)
\end{bmatrix}</script><p>下面进行线性方程的矩阵直接求解：</p>
<script type="math/tex; mode=display">
\begin{align}
v &= \mathcal{R}+\gamma \mathcal{P}v\\
(1-\gamma \mathcal{P})&= \mathcal{R}\\
v &= (1-\gamma \mathcal{P})\mathcal{R}
\end{align}</script><p>当然这种直接解法只能适用于小型的MRP模型，大型的MRP模型通常采用迭代的方法，比如动态规划，蒙特卡洛评估，时序差分学习等等。</p>
<h2 id="4-Markov-Decision-Process马尔可夫决策过程"><a href="#4-Markov-Decision-Process马尔可夫决策过程" class="headerlink" title="4 Markov Decision Process马尔可夫决策过程"></a>4 Markov Decision Process马尔可夫决策过程</h2><h3 id="4-1-MDP"><a href="#4-1-MDP" class="headerlink" title="4.1 MDP"></a>4.1 MDP</h3><p>下面终于讲到了今天的重头戏，MDP模型，如模型标题的意思所言，MDP就是具有决策状态的马尔可夫奖励过程。这里我们直接给出了马尔可夫决策过程的定义：$A\; Markov\; Decision\; Process\; is \; a\; tuple\; \langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\mathcal{\gamma} \rangle$</p>
<p>显然比起马尔可夫奖励过程，我们多了一个$\mathcal{A}$集合代表决策过程中所有action的集合。</p>
<p>而相应的，我们也需要改动传输概率矩阵和奖励函数的定义式了，因为他们都需要与action有关了。</p>
<script type="math/tex; mode=display">
\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s'\; |\; S_t=s,A_t=a]\\
\mathcal{R}_{s}^a = \mathbb{E}[R_{t+1}\; |\; S_t=s,A_t=a]</script><p>下面让我们回到那个学生上课的例子：</p>
<p><img src="/../image/markov3.png" alt=""></p>
<p>在MDP模型中，我们的智能体agent是能选择自己行动的action的，如果环境因为他的action变差了，那它就会因此背锅，最终的选择一定是让环境越来越好。</p>
<h3 id="4-2-Policies策略"><a href="#4-2-Policies策略" class="headerlink" title="4.2 Policies策略"></a>4.2 Policies策略</h3><p>策略是agent对于环境所表达的行为，这里我们给出它的定义和解释：</p>
<script type="math/tex; mode=display">
\pi(a|s)=\mathbb{P}[A_t=a\;|\; S_t=s]</script><p>在上述定义中，MDP模型依旧是所有状态保持马尔可夫性，则我们可以得出MDP的策略也只与当前状态有关，与历史无关。另外策略也是与时间无关的，仅与当前状态有关，即$A_t\sim \pi(\cdot|S_t),\forall t&gt;0$</p>
<p>下面我们来理解策略在MDP的作用，以及MDP模型的分解：</p>
<ul>
<li>给出一个MDP $\mathcal{M}=\langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\mathcal{\gamma} \rangle$ 和一个策略 $\pi$</li>
<li>此时状态序列就构成了马尔可夫链 $\langle \mathcal{S},\mathcal{P}^\pi \rangle$</li>
<li>此时状态与奖励构成了MRP $\langle \mathcal{S},\mathcal{P}^\pi,\mathcal{R}^\pi,\mathcal{\gamma} \rangle$</li>
</ul>
<p>而这里我们给出基于策略 $\pi$ 的传输概率矩阵与奖励，即新的定义：</p>
<script type="math/tex; mode=display">
\mathcal{P}_{ss'}^{\pi}=\sum_{a\in \mathcal{A}}\pi(a|s)\mathcal{P}_{ss'}^a\\
\mathcal{R}_s^{\pi} = \sum_{a\in \mathcal{A} }\pi(a|s)R_s^a</script><h3 id="4-3-Policy-based-Value-Function基于策略的价值函数"><a href="#4-3-Policy-based-Value-Function基于策略的价值函数" class="headerlink" title="4.3 Policy based Value Function基于策略的价值函数"></a>4.3 Policy based Value Function基于策略的价值函数</h3><p>MDP模型中有两种基于策略的价值函数：(1) 在状态s时收益的期望，代表的是状态带来的价值 (2) 在状态s时，采取动作a后收益的期望，代表的是动作带来的价值。两者共同构成了MDP的价值函数。</p>
<p>下面我们分别给出定义式：</p>
<ul>
<li>state-value function</li>
</ul>
<script type="math/tex; mode=display">
v_{\pi}(s)=\mathbb{E}_\pi[G_t\; |\; S_t=s]</script><ul>
<li>action-value function</li>
</ul>
<script type="math/tex; mode=display">
q_\pi(s,a)=\mathbb{E}_\pi[G_t\; |\; S_t=s,A_t=a]</script><h3 id="4-4-Bellman-Expectation-Equation贝尔曼期望方程"><a href="#4-4-Bellman-Expectation-Equation贝尔曼期望方程" class="headerlink" title="4.4 Bellman Expectation Equation贝尔曼期望方程"></a>4.4 Bellman Expectation Equation贝尔曼期望方程</h3><p>同样，我们依旧可以利用贝尔曼方程来转换两个基于策略的价值函数：</p>
<script type="math/tex; mode=display">
v_{\pi}(s)=\mathbb{E}_\pi[R_{t+1} + \gamma v_\pi (S_{t+1})\; |\; S_t=s]\\
q_\pi(s,a)=\mathbb{E}_\pi[R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1})\; |\; S_t=s,A_t=a]</script><p>下面我们需要将我们的MDP模型进行解释，在马尔可夫链和MRP模型中，我们仅有状态的价值函数，那很显然在MDP模型中这是不够的，因为我们引入了Action，如果环境因为我们agent的Action而变差了，那Action则一定要背锅，于是我们也要考虑Action的价值函数，下面我们根据各种情况分别阐述如何求出价值函数。</p>
<ul>
<li>从状态到动作的价值函数</li>
</ul>
<p><img src="/../image/markov4.png" alt=""></p>
<p>当我们处于状态s时，agent有两个状态可以去执行，那我们此处的价值函数可以定义为：</p>
<script type="math/tex; mode=display">
v_\pi(s)=\sum_{a\in \mathcal{A}}\pi(a|s)q_\pi(s,a)</script><p>这其实也很好理解，状态的价值函数就是所有下一步执行动作的价值函数的数学期望。</p>
<ul>
<li>从动作到状态的价值函数</li>
</ul>
<p><img src="/../image/markov5.png" alt=""></p>
<p>当从状态s执行动作action后我们可以进入下一个状态s‘，那我们此处的价值函数可以定义为：</p>
<script type="math/tex; mode=display">
q_\pi(s,a)=\mathcal{R}_s^a+\gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s')</script><p>动作的价值函数就是离开状态s的即时奖励，加上所有可以进入下一个状态概率与价值的和。</p>
<ul>
<li>从状态到状态的价值函数</li>
</ul>
<p><img src="/../image/markov6.png" alt=""></p>
<p>状态到状态的价值函数只需要把上面两个价值函数合在一块即可：</p>
<script type="math/tex; mode=display">
v_\pi(s)=\sum_{a\in \mathcal{A}}\pi(a|s)\left(\mathcal{R}_s^a+\gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s')\right)</script><ul>
<li>从动作到动作的价值函数</li>
</ul>
<p><img src="/../image/markov7.png" alt=""></p>
<script type="math/tex; mode=display">
q_\pi(s,a)=\mathcal{R}_s^a+\gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a\sum_{a'\in \mathcal{A}}\pi(a'|s')q_\pi(s',a')</script><p>下面举一个简单的计算例子，还是使用学生上课的MDP模型。</p>
<p><img src="/../image/markov8.png" alt=""></p>
<p>假设我们通过某些算法求得红色的状态的价值为7.4，那我们该如何进行验证？这里可能会有读者很奇怪，这个价值函数不是求出来的嘛，为啥是验证。这些价值函数都只能得到一个近似解，那么所有近似解都涉及到了损失值的问题，那我们就需要去拟合最小的Loss，当然现在流行深度强化学习方法。</p>
<p>好，话说回来，根据我们上面状态到状态的价值函数，就可以很明确的得出这个值了，计算公式如图红色部分所示。下面我们列出MDP模型的贝尔曼方程矩阵形式，并给出直接解。</p>
<script type="math/tex; mode=display">
v_\pi=\mathcal{R}^\pi+\gamma \mathcal{P}^\pi v_\pi \\
v_\pi =(1-\gamma \mathcal{P}^\pi)^{-1}\mathcal{R}^\pi</script><h3 id="4-5-Optimal-Value-Function最优价值函数"><a href="#4-5-Optimal-Value-Function最优价值函数" class="headerlink" title="4.5 Optimal Value Function最优价值函数"></a>4.5 Optimal Value Function最优价值函数</h3><p>下面我们分别给出状态和动作最优价值函数的定义，其实就是在策略 $\pi$ 下，可以取得最大的价值函数，因为由于agent选择的不同策略，所有状态的价值函数都会相应的改变。</p>
<script type="math/tex; mode=display">
v_*(s)=\max_\pi v_\pi(s)\\
q_*(s,a)=\max_\pi q_\pi(s,a)</script><p>所有的MDP模型的最终任务就是为了确定最优价值函数相应的策略。</p>
<h3 id="4-6-Theorem-of-MDP定理"><a href="#4-6-Theorem-of-MDP定理" class="headerlink" title="4.6 Theorem of MDP定理"></a>4.6 Theorem of MDP定理</h3><p>以上给出的绝大部分是定义，下面我们给出几条MDP模型的定理，注意这是定理。</p>
<ul>
<li>对于MDP模型，我们承认存在一个最优策略$\pi_<em>$，比其他任何策略都好，即$\pi_</em> \geq \pi, \forall \pi$，这里我们还需要再定义策略比较的规则，即$\pi \geq \pi’ \; if\; v_\pi(s)\geq v_{\pi’}(s),\forall s$ </li>
<li>所有的最优策略都有相同的最优价值函数，即$v_{\pi_<em>}(s)=v_</em>(s)$</li>
<li>所有的最优策略都有相同的最优动作价值函数，即$q_{\pi_<em>}(s,a)=q_{</em>}(s,a)$</li>
</ul>
<h3 id="4-7-Finding-an-Optimal-Policy寻找最优策略"><a href="#4-7-Finding-an-Optimal-Policy寻找最优策略" class="headerlink" title="4.7 Finding an Optimal Policy寻找最优策略"></a>4.7 Finding an Optimal Policy寻找最优策略</h3><p>我们可以通过最大化动作价值函数$q_{*}(s,a)$的方法来寻找最优策略：</p>
<script type="math/tex; mode=display">
\pi_*(a|s)=\left\{
\begin{array}{rl}
1 & \text{if } \displaystyle a=\arg\max_{a\in \mathcal{A}} q_*(s,a)\\
0 & \text{otherwise } 
\end{array} \right.</script><h3 id="4-8-Bellman-Optimality-Equation贝尔曼最优方程"><a href="#4-8-Bellman-Optimality-Equation贝尔曼最优方程" class="headerlink" title="4.8 Bellman Optimality Equation贝尔曼最优方程"></a>4.8 Bellman Optimality Equation贝尔曼最优方程</h3><p><img src="/../image/markov9.png" alt=""></p>
<p>当我们的agent在状态s时，对于该状态的最优价值函数一定是选择价值最优的动作函数，即：</p>
<script type="math/tex; mode=display">
v_*(s)=\max_a q_*(s,a)</script><p>而对于action的最优动作价值而言，相当于离开状态s的即时奖励，加上可以转移的下一个状态的最优价值与传输概率的成绩之和，即：</p>
<p><img src="/../image/markov10.png" alt=""></p>
<script type="math/tex; mode=display">
q_*(s,a)=\mathcal{R}_s^a+\gamma\sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^av_*(s')</script><p>那么状态到状态的价值函数与动作到动作的价值函数与上面同理，只需要把两个方程式叠加即可。</p>
<p><img src="/../image/markov11.png" alt=""></p>
<p><img src="/../image/markov12.png" alt=""></p>
<p>最后再举一个计算最优方程的例子，让我们再回到学生上课的MDP模型中。</p>
<p><img src="/../image/markov13.png" alt=""></p>
<p>相信如果你认真看完了以上内容，能很好地明白红色的式子，当然这里我们假设了执行a动作时，必然会进入下一个状态s‘，即传输概率为1。</p>
<h3 id="4-9-Solving-the-Bellman-Optimality-Equation求解贝尔曼最优方程"><a href="#4-9-Solving-the-Bellman-Optimality-Equation求解贝尔曼最优方程" class="headerlink" title="4.9 Solving the Bellman Optimality Equation求解贝尔曼最优方程"></a>4.9 Solving the Bellman Optimality Equation求解贝尔曼最优方程</h3><p>贝尔曼最优方程是非线性的，通常而言没有固定的解法，有很多著名的迭代解法：</p>
<ul>
<li>Value Iteration 价值迭代</li>
<li>Policy Iteration 策略迭代</li>
<li>Q-learning </li>
<li>Sarsa</li>
</ul>
<p>这个可以大家之后去多了解了解。</p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>最后做个总结的话，这个应该是我研究最久的模型了，里面可能有很不到位的讲解之处，请大家谅解。</p>
<p>另外有任何问题，欢迎评论区留言来一起交流 ~ </p>

    </div>

    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

            <div class="social-item">
              <a target="_blank" class="social-link" href="/atom.xml">
                <span class="icon">
                  <i class="fa fa-rss"></i>
                </span>

                <span class="label">RSS</span>
              </a>
            </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" rel="tag"># 马尔可夫</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/18/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Anomaly-Detection%E7%BB%BC%E8%BF%B0/" rel="prev" title="异常检测|Anomaly Detection综述">
      <i class="fa fa-chevron-left"></i> 异常检测|Anomaly Detection综述
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/10/26/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7%E6%96%B9%E5%90%918%E7%AF%87%E8%AE%BA%E6%96%87%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/" rel="next" title="异常检测|视频监控方向8篇论文模型浅析">
      异常检测|视频监控方向8篇论文模型浅析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-number">1.</span> <span class="nav-text">1 Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Markov-Processes马尔可夫过程"><span class="nav-number">2.</span> <span class="nav-text">2 Markov Processes马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Markov-Property马尔可夫性"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Markov Property马尔可夫性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-State-Transition-Matrix状态传输矩阵"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 State Transition Matrix状态传输矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Markov-Chain马尔可夫链"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Markov Chain马尔可夫链</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Markov-Reward-Process马尔可夫奖励过程"><span class="nav-number">3.</span> <span class="nav-text">3 Markov Reward Process马尔可夫奖励过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-MRP"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 MRP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Return回报"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Return回报</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Value-Function价值函数"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Value Function价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Bellman-Equation贝尔曼方程"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 Bellman Equation贝尔曼方程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Markov-Decision-Process马尔可夫决策过程"><span class="nav-number">4.</span> <span class="nav-text">4 Markov Decision Process马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-MDP"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 MDP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Policies策略"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Policies策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Policy-based-Value-Function基于策略的价值函数"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 Policy based Value Function基于策略的价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Bellman-Expectation-Equation贝尔曼期望方程"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 Bellman Expectation Equation贝尔曼期望方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-Optimal-Value-Function最优价值函数"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 Optimal Value Function最优价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-Theorem-of-MDP定理"><span class="nav-number">4.6.</span> <span class="nav-text">4.6 Theorem of MDP定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-Finding-an-Optimal-Policy寻找最优策略"><span class="nav-number">4.7.</span> <span class="nav-text">4.7 Finding an Optimal Policy寻找最优策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-8-Bellman-Optimality-Equation贝尔曼最优方程"><span class="nav-number">4.8.</span> <span class="nav-text">4.8 Bellman Optimality Equation贝尔曼最优方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-9-Solving-the-Bellman-Optimality-Equation求解贝尔曼最优方程"><span class="nav-number">4.9.</span> <span class="nav-text">4.9 Solving the Bellman Optimality Equation求解贝尔曼最优方程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Conclusion"><span class="nav-number">5.</span> <span class="nav-text">5 Conclusion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Alpha Yang"
      src="/images/IMG_1979(20200206-160439).JPG">
  <p class="site-author-name" itemprop="name">Alpha Yang</p>
  <div class="site-description" itemprop="description">世间万物，唯代码美食，不能辜负。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Alpha-Yang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Alpha-Yang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>


<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
      </div>
	  
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alpha Yang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <script async src="/js/cursor/fireworks.js"></script>



</body>
</html>
